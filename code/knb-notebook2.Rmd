---
title: "KNB Notebook"
output: github_document
---
# Scope of the database
What are the measurements? Are there variables that are required for each data point? Try to explain in a programmatic way.

Since KNB has multiple datasets, it has info of all types of animals, including birds, fish, mammals, etc. Depending on the dataset, it has different types of variables and measurements. 

Take one dataset for example, which is the one I downloaded in the first notebook,
```{r}
river <- read.csv("~/Documents/nahis/knb-susquehanna-river-flow.csv")
str(river)
river
```

flow: cubic feet per second  
a: USGS gage number [google "1570500 usgs"](https://pubs.usgs.gov/of/2016/1038/ofr20161038.pdf)  


Another dataset
```{r}
mya <- read.csv("~/Documents/nahis/knb.92072.1.csv")
str(mya)
mya

# columns that have no NA's
table(factor(which(!is.na(mya), arr.ind=TRUE)[, 2]))
```


# Data Quality
I don't see any documentation on the standards. Units and abbreviations are quite vague. I have to go to the original site to refer to other similar datasets. Or google some unique data entries, such as IDs. 

But if it's about river, we can check the climate there and get a sense of rainfall and water stream, etc. Also for animal sizes, we can compare with their avarage sizes to detect possible outliers.

## Completeness
> requires that a particular column, element or class of data is populated and does not feature null values or values in place of nulls (e.g. N/As).



```{r}
na1 <- which(is.na(river), arr.ind=TRUE)
na2 <- which(is.na(mya), arr.ind=TRUE)
```

## Consistency
> Something that tests whether one fact is consistent with another e.g. gender and title in a CRM database.

We can check seasons and dates/datas and years. 

## Uniqueness
> Are all the entities or attributes within a dataset unique?

By now I haven't seen any duplicate attributes. Also, since I have data frames, I think all the attributes should be unique and otherwise R will change them automatically.

## Integrity
> Are all the relationships populated for a particular entity â€“ for example its parent or child entities?

No. For crabs and fish, we only have their lengths. 

## Conformity
> Does the data conform to the right conventions and standards. For example a value may be correct but follow the wrong format or recognised standard.

The units are generally unclear. 

## Accuracy
> the hardest dimension to test for as this often requires some kind of manual checking by a Subject Matter Expert (SME).

I think data coming from a governmental institute or a university is pretty accurate.

# What are the variables that are most interesting to you?
> At some point you will need to refine the scope of your project. You likely cannot explore ALL the data. Are their questions about the that are particularly interesting to you? Questions can either be about the quality of the data or of biological significance.

Right now I'm thinking about how climate changes affect one species' living conditions. Or maybe just river flows? 
I think biological significance sounds pretty interesting but I'm not sure if there are a lot of things to be done. Quality of data is also good to study as well... 

# Skills
> Reiterate what skills you particullarly interested in learning. Do you see a clear path from this database to level up on those skills?

I'm most interesting in machine learning.  
If I do the quality of data, I think I can do something similar to what I did in a Data8 project, which is making a test set and compare the data with the set to determine the outliers. And I can explore more on that, such as advanced testing methods.
On the other hand, biological significance will lead me to a data visualization path, I think. I will compare different pairs of data attributes and find their relationships?

# Handle the data
Is there something that could be done to the data on the database side that would make your life easier when using this data? Do you wish it was in json over XML? Do you wish that there was a tool in Python that would connect to the database? Did you find the documentation incredibly hard to follow? What are some things you googled that helped you? What are the things you googled that had no answer but wish there was?

I'm actually able to read XML now using Atom because it colors different nodes. I also changed some settings to "soft wrap at preffered line length" and it makes xml way easier to read! I'm almost in love with it!  
Useful guide to limit line lengths in atom [here](https://stackoverflow.com/questions/49616864/limiting-line-length-in-atom)  

As for some data that is stored in the xml file, I can extract that part and use the `xmlToDataFrame` method in `XML` package. [tutorial mentioned last time](https://www.youtube.com/watch?v=1cM_ZNZ9hhE).


Ciera suggested that looking for the most popular headers of all the datasets should be fun. I thought that would be interesting but had no idea where to start. So I went to the KNB website and joined their Slack channel. One of the staff, Matt Jones, helped get all the attributes of 69957 datasets, extracted from their metadata files using their API. 

```{r, eval = FALSE}
# load the package
library(dataone)

# set to the KNB database
cn <- CNode("PROD")
mn <- getMNode(cn, "urn:node:KNB")

# this gives a data frame of the first 1000 metadata files 
# with only their ID and attribute information
all <-
  list(
    q = "formatType:METADATA+AND+attributeName:*",
    fl = "identifier,formatId,attributeName,attributeDescription",
    wt = "xml",
    rows = "1000",
    start = "0"
  )
allre <- query(mn, solrQuery = all, as = "data.frame")
```

Then I used a for loop to merged the 70 data frames and the resulting data frame is `allre`, saved as a csv file. This takes really long.


```{r, eval = FALSE}
# load the package
library(dataone)

# set to the KNB database
cn <- CNode("PROD")
mn <- getMNode(cn, "urn:node:KNB")

allre <- data.frame()
for (i in seq(0, 69000, 1000)) {
  all <-
  list(
    q = "formatType:METADATA+AND+attributeName:*",
    fl = "identifier,formatId,attributeName,attributeDescription",
    wt = "xml",
    rows = "1000",
    start = paste0(i)
  )
  allre <- rbind(allre, query(mn, solrQuery = all, as = "data.frame"))
}
write.csv(allre, file = "~/Documents/nahis/knb-attrs.csv")
```

Next I extract all the attribute names, in lower case, in the data frame and put them into one vector, `attrs`.

Of course, some attributes may have different names even though they mean the same thing as other attributes. For example, some datasets may have an attribute names "len", which is the same as "length"; or some may have abbreviations. And this may be harmful to this exploration. So we may want to change the similar attribute

```{r}
# this function gives all the attribute names for one metafile, ie, one row of allre
get_attr <- function(x) {
  tolower(unlist(strsplit(allre$attributeName[x], "\\s+")))
}
attrs <- sapply(1:69957, get_attr, simplify=TRUE)
```

Now I make a frequency table of all the attribute names and make a barchart of them in descending order of frequency. Thus we can see which attributes are the most popular.  

```{r, eval = FALSE}
attr_tab <- table(factor(unlist(attrs)))
most_pop <- sort(attr_tab, decreasing=TRUE)
write.csv(most_pop, file = "~/Documents/nahis/knb-pop-attrs.csv")
```

```{r, plot1}
pop_df <- data.frame(freq = as.numeric(unname(most_pop)), attr = names(most_pop))
pop_df <- head(pop_df, 50)
library(ggplot2)
ggplot(data=pop_df, aes(x=freq, y=attr)) +
  geom_bar(stat="identity", fill="light blue", descending = TRUE)
  
```

So we see that the first five attributes don't have any special meanings. They just represend time/data. So let's look at 6th to 10th 
```{r, plot2}
barplot(most_pop[6:20], col = "light blue", horiz=TRUE, las=1)
```
