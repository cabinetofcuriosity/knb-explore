---
title: "Location"
output: github_document
---
This document extracts location and date information from PISCO, species and sea star wasting syndrome datasets and compare them. 

```{r setup, include=FALSE}
d203 <- read.csv("../data/d203.csv", stringsAsFactors = FALSE)
dataset_w_pop <- read.csv("../data/pop_ds.csv", stringsAsFactors = FALSE)
allLocationNDate <- read.csv("../data/pisco-locations-dates.csv", stringsAsFactors = FALSE)
ca_which_pis <- read.csv("../data/ca_sea_star_vs_pisco.csv", stringsAsFactors = FALSE)
```


```{r}
# Load some packages and import some data frames
library(ggplot2)
library(dplyr)
```


## Photo plots

> PermanentPhoto plot methods photo photoplots are used to monitor percent cover of organisms within target species assemblages. Plots are established for a given species assemblage if overall cover at a site is sufficient for monitoring. In general, 5 replicate plots (50 x 75 cm) are placed in a stratified random manner throughout the target species' zone of maximum abundance.  
Plots are photographed in the field, and are scored for percent cover using a grid of 100 points, either in the field or in the lab from photos. A species, higher taxon, or substrate located below each of the 100 points is identified and recorded. Layering is not generally scored separately, so the total cover is 100 percent. However, there is an optional method for sampling photo plots that includes layering and spatial position. With this optional method, the top and bottom layers are recorded for each location on the grid. This provides extra data that can be used to gather information on species associations and small scale spatial changes over time.

So first we get the [photo plots data](https://www.eeb.ucsc.edu/pacificrockyintertidal/explore-the-data/index.html). We can see the data is inside four states.

```{r, eval=FALSE}
sea_stars <- read.csv("../data/phototranraw_download.csv", stringsAsFactors = FALSE)
unique(sea_stars$state_province)
```

We first examine the species data inside California and later to all the data.
```{r, eval=FALSE}
sea_stars_ca <- sea_stars[sea_stars$state_province == 'California', ]
# add ids to represent each row
sea_stars_ca <- mutate(sea_stars_ca, id = 1:nrow(sea_stars_ca))
head(sea_stars_ca, 100)
colnames(sea_stars_ca)
dim(sea_stars_ca)

sea_stars_ca %>%
  group_by(target_assemblage, marine_common_year) %>%
  count()

sea_stars_ca %>%
  group_by(plot_code, marine_site_name) %>%
  count()

### Mapping species onto ggmap
### Somehow looking at temperature and species distribution. Maybe add the temperature on the map as a color?

## Subset by year first.
## plot each species by plot code and site name --- dots to line shape --- then with pisco ---

sea_stars_ca %>%
  filter(marine_site_name == "Stairs", plot_code == 1, target_assemblage == "endocladia", marine_common_year == 2002) %>%
  ggplot(., aes(survey_date, percent_cover, color = target_assemblage)) +
  geom_point() +
  theme_bw()

```

```{r, eval=FALSE}
if(!requireNamespace("devtools")) install.packages("devtools")
devtools::install_github("dkahle/ggmap", ref = "tidyup", force=TRUE)

# Load ggmap
library("ggmap")

# Set your API Key
ggmap::register_google(key = "YOUR KEY")
```

We can see plenty of repeating latitudes and longitudes so we can extract the unique pairs of latitude and longitude, ie, unique locations and a survey date with an average percent_cover of that day.
```{r, eval=FALSE}
ca_unique_ld <- sea_stars_ca %>% 
  group_by(longitude, latitude, survey_date, target_assemblage) %>%
  summarise(mean_percent = mean(percent_cover))

sea_stars_ca %>% 
  group_by(longitude, latitude, survey_date, target_assemblage) %>%
  summarise(mean_percent = mean(percent_cover)) %>%
  ggplot(., aes(survey_date, mean_percent)) +
  geom_point()

ca_unique_ld
```


```{r, eval=FALSE}
unique(sea_stars_ca[, 'target_assemblage'])
unique(sea_stars_ca[, 'mpa_region'])
```

## PISCO
Some setup for getting the location data.
```{r, eval = FALSE}
library(dataone)
library(XML)

id <- dataset_w_pop[1, 3]
cn <- CNode("PROD")

# get the node of this metadata using `dataOne` package
locations <- resolve(cn, id)
mnId <- locations$data[2, "nodeIdentifier"]
mn <- getMNode(cn, mnId)
```

Define a function which returns location and date information of the ith PISCO datasets.
```{r, eval = FALSE}
getLocationNDate <- function(i) {
  # id of the ith row of the PISCO datasets
  id <- dataset_w_pop[i, 3]

  # download the metadata file to find the data table
  metadata <- rawToChar(getObject(mn, id))
  doc = xmlRoot(xmlTreeParse(metadata, asText=TRUE, trim = TRUE, ignoreBlanks = TRUE))

  # now extract the node that has the data table's location information
  node <- getNodeSet(doc, "//boundingCoordinates")
  curLoc <- head(xmlToDataFrame(node, stringsAsFactors = FALSE), 1)
  west <- as.numeric(curLoc[1, 1])
  east <- as.numeric(curLoc[1, 2])
  north <- as.numeric(curLoc[1, 3])
  south <- as.numeric(curLoc[1, 4])
  
  begin_node <- getNodeSet(doc, "//beginDate")
  begin_date <- xmlToDataFrame(begin_node, stringsAsFactors = FALSE)[1, 1]
  end_node <- getNodeSet(doc, "//endDate")
  end_date <- xmlToDataFrame(end_node, stringsAsFactors = FALSE)[1, 1]
  
  # take the average of west and east, and north and south
  thisLoc <- data.frame(longitude = c((1/2)*(west + east)), latitude = c((1/2)*(north + south)), begin = c(begin_date), end = c(end_date), ID = c(id))
  
  return(thisLoc)
}
```
```{r, eval=FALSE}
getLocationNDate(926)
```
> Error in .local(x, ...) : get() error: READ not allowed on doi:10.6085/AA/JALXXX_015ADCP015R00_20030529.50.2

After testing, 926th ID doesn't work

```{r, eval = FALSE}
allLocationNDate <- data.frame()
for (i in 1:925) {
  allLocationNDate <- rbind(allLocationNDate, getLocationNDate(i))
}

for (i in 927:nrow(dataset_w_pop)) {
  allLocationNDate <- rbind(allLocationNDate, getLocationNDate(i))
}
```

We save this data frame into a `.csv` file.
```{r, eval = FALSE}
write.csv(allLocationNDate, '../data/pisco-locations-dates.csv')
```


## Find the nearest PISCO site for each species data entry  
Define a function that takes in index `i`, sea stars data `sea_star_dt` and PISCO data `pisco_dt` and returns the row number of the neareast PISCO data

```{r}
head(allLocationNDate, 10)
```


Since there are begin and end dates in PISCO datasets, we want to transform the dates into numbers so that we can compare the dates and determine if a data entry in the sea star dataset can find a corresponding dataset in PISCO ones. 
Now we define a function to do so, using method found on [stackoverflow](https://stackoverflow.com/a/8215581/10733819)
```{r}
dateToNum <- function(date) {
  rt <- 0
  date <- as.POSIXlt(date, format = "%Y-%m-%d")
  # yearday will always be less than or equal to 366 so we can represent yeardays as decimal numbers, and years as whole numbers
  rt <- 1900 + date$year + date$yday / 366
  return(rt)
}
```

```{r}
dateToNum('0000-02-29')
dateToNum('2008-02-29')
```


We put in a data frame of species with unique pairs of locations(latitude and longitude) and dates and find their corresponding pisco datasets, using the Euclidean distance and date. If the minimum Euclidean distance is bigger than 1 or the dates do not match, I assume this photo plot data entry doesn't have a corresponding PISCO dataset. 
```{r, eval=FALSE}
nearPisWDate <- function(i, sea_star_dt, pisco_dt) {
  ss <- sea_star_dt[i, ]
  ss_loc <- c(ss$longitude, ss$latitude)
  # change the date into a number
  ss_date_n <- dateToNum(ss$survey_date)
  
  min_dis <- 100000
  which_pis <- -1
  for (j in 1:nrow(pisco_dt)) {
    cur_pisco <- pisco_dt[j, ]
    cur_pisco_date <- c(cur_pisco$begin, cur_pisco$end)
    cur_pisco_begin <- dateToNum(cur_pisco_date[1])
    cur_pisco_end <- dateToNum(cur_pisco_date[2])
    if (ss_date_n > cur_pisco_end | ss_date_n < cur_pisco_begin) {
      next
    }
    
    cur_pisco_loc <- c(cur_pisco$longitude, cur_pisco$latitude)
    dis <- (ss_loc[1] - cur_pisco_loc[1])**2 + (ss_loc[2] - cur_pisco_loc[2])**2
    if (dis < min_dis) {
      min_dis <- dis
      which_pis <- j
    }
  }
  if (min_dis > 2) {
    return(-1)
  }
  #result <- list("PISCO" = which_pis, "distance" = min_dis)
  return(which_pis)
}
```

```{r, eval=FALSE}
ca_which_pis <- c()
for (m in 1:nrow(ca_unique_ld)) {
  ca_which_pis <- c(ca_which_pis, nearPisWDate(m, ca_unique_ld, allLocationNDate))
}
write.csv(ca_which_pis, '../data/ca_sea_star_vs_pisco.csv')
```


Next we define a function to combine species datasets and their corrsponding PISCO datasets. 
```{r, eval=FALSE}
find_the_pisco <- function(df, data_list) {
  rt <- data.frame()
  for (i in 1:length(data_list)) {
    cur = data_list[i]
    if (cur == -1) {
      next
    }
    cur <- data.frame(latitude = df$latitude[cur], longitude = df$longitude[cur], begin = df$begin[cur], end = df$end[cur], ID = df$ID[cur], species_ind = c(i), pis_ind = c(cur))
    rt <- rbind(rt, cur)
  }
  return(rt)
}
```
```{r, eval=FALSE}
species_pis <- find_the_pisco(allLocationNDate, ca_which_pis$x)
species_pis
```
```{r, eval=FALSE}
write.csv(species_pis, '../data/species_which_pisco.csv')
```

```{r, eval=FALSE}
library(dataone)
library(XML)
locations <- resolve(cn, id)
mnId <- locations$data[2, "nodeIdentifier"]
mn <- getMNode(cn, mnId)
```

For each data entry of `species_pis`, extract the temprature information from its corresponding PISCO dataset. 
Define a function to download a PISCO dataset by its index in `dataset_w_pop`.
```{r, eval=FALSE}
downl_pis <- function(i) {
  id <- dataset_w_pop[i, 3]

  # download the metadata file to find the data table
  metadata <- rawToChar(getObject(mn, id))
  doc = xmlRoot(xmlTreeParse(metadata, asText=TRUE, trim = TRUE, ignoreBlanks = TRUE))

  # now extract the node that has the data table's information
  node <- getNodeSet(doc, "//entityName")
  table_id <- xmlValue(node[[1]])
  
  if (grepl("\\.(TXT|txt)", table_id)) {
    table_id <- gsub("\\.(TXT|txt)", "", table_id)
  }
  
  # we can see that the ids have the pattern
  dataRaw <- getObject(mn, paste0("doi:10.6085/AA/", table_id))
  dataChar <- rawToChar(dataRaw)
  theData <- textConnection(dataChar)
  df <- read.csv(theData, stringsAsFactors=FALSE, header = TRUE, sep = " ", row.names=NULL)
  return(df)
}
```


```{r, eval=FALSE}
get_temp <- function(spe_pis, spe_dt) {
  spe_ind <- species_pis$species_ind
  pis_ind <- species_pis$pis_ind
  result <- data.frame()

  for (i in 1:nrow(species_pis)) {
    cur_spe <- spe_ind[i] # index of spe_dt 
    cur_pis <- pis_ind[i] # index of pisco datasets
    if (cur_pis >= 926) {
      cur_pis <- cur_pis + 1
    }
    
    pisco_df <- downl_pis(cur_pis)
    ### if you've downloaded all the datasets:
    ### pisco_df <- read.csv(file = paste0("../data/d", cur_pis, ".csv"), stringsAsFactors = FALSE)
    
    # discard if temp == 9999.00, which is equivalent to NA for PISCO datasets
    tem <- filter(pisco_df, date == spe_dt$survey_date[cur_spe], temp_c != 9999.00)
    result <- rbind(result, average(tem))
  }
}
```

```{r, eval=FALSE}
dt1_tem <- filter(dt1, date == '2002-11-03', temp_c != 9999.00)
unique(dt1$date)
ggplot(data = dt1_tem, aes(x=X, y=temp_c)) +
  geom_bar(stat="identity", fill = "lightblue") +
  theme_minimal()
```
As we see from the plot above, ocean temperature doesn't tend to vary a lot, for which one of the reasons is that water has a high "specific heat capacity". Hence I would prefer to see a long-term trend of ocean temperature and its relation to species data.   

# Sea Star Wasting Syndrome  
I got the [sea star wasting syndrome](https://www.eeb.ucsc.edu/pacificrockyintertidal/explore-the-data/index.html) data from one of the staff via email. 
```{r}
ssws <- read.csv("../data/sswd_sea_star_observations_2019_0411.csv", stringsAsFactors = FALSE)
```
However, the wasting syndrome data has a different time range than the PISCO data:
```{r}
pisco_mean_date <- c()
for (n in 1:length(allLocationNDate$begin)) {
  pisco_mean_date <- c(pisco_mean_date, (dateToNum(allLocationNDate$end[n]) + dateToNum(allLocationNDate$begin[n]))/2)
}
max(pisco_mean_date)
min(pisco_mean_date)
```

```{r}
ssws_date <- c()
for (n in 1:length(ssws$sample_date)) {
  ssws_date <- c(ssws_date, dateToNum(ssws$sample_date[n]))
}
max(ssws_date)
min(ssws_date)
```
Hence I may need to save this dataset for later.


# 



```{r, eval=FALSE}
ca_merge <- sea_stars_ca %>% 
  group_by(lumping_code, marine_common_year, target_assemblage) %>%
  summarise(mean_percent = mean(percent_cover))
ca_merge
```
```{r, eval=FALSE}
ca_merge %>%
  filter(lumping_code == 'ANTELE', target_assemblage =='balanus') %>%
  ggplot(aes(marine_common_year, mean_percent)) + 
  geom_line() +
  theme_minimal()
  
```


I realize that taking the mean of percent_cover over one survey day doesn't make sense with temperature data for that day, since percent_cover is recorded for several times within a day 
